{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\"\"\"\n",
    "<br>@Project: LTRO Project - Team 4 - Forecast for multiple location and all types of waste\n",
    "<br>@author: Harihareshwar Kumaravel\n",
    "<br>\"\"\"***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Forecasting Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction Code\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.tsa.statespace.sarimax as sarimax\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ///INPUTS///\n",
    "\n",
    "\n",
    "p = 'BX'\n",
    "q = 'Verre'\n",
    "r = \"OM\"\n",
    "s = \"Carton\"\n",
    "c = []\n",
    "loc = \"Data cleared.xlsx\"  # Location where the cleaned data is present\n",
    "'''d = '191' #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "m = 1 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively'''\n",
    "\n",
    "# ///FUNCTIONS///\n",
    "\n",
    "def SelectPoints(point, m):\n",
    "    df = pd.read_excel(loc , sheet_name= m)\n",
    "    df = df[df['date'] >= '01-01-2018']\n",
    "    df['n_point']= df['n_point'].astype(str)\n",
    "    df = df[df[\"n_point\"] == point]\n",
    "    df['taux'] = df['taux'].fillna(value = np.mean(df['taux']))\n",
    "    df = df.query('taux <= 1.0')\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "    df['day'] = [i.day for i in df['date']]\n",
    "    df['month'] = [i.month for i in df['date']]\n",
    "    df['year'] = [i.year for i in df['date']]\n",
    "    df['week'] = [i.week for i in df['date']]\n",
    "    if m == 0:\n",
    "        for i in range(10):\n",
    "            c.append(p)\n",
    "    elif m == 1:\n",
    "        for i in range(10):\n",
    "            c.append(q) \n",
    "    elif m == 1:\n",
    "        for i in range(10):\n",
    "            c.append(r) \n",
    "    else:\n",
    "        for i in range(10):\n",
    "            c.append(s)\n",
    "    return df\n",
    "\n",
    "#n = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "#Select the point to be forecasted and the type of waste. \n",
    "'''df = SelectPoints(d,m)  \n",
    "df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "n = df['days before last pick up'].mean(skipna=True)'''\n",
    "\n",
    "#Spliting the data to evaluate the models using MSE(Mean Squared error)\n",
    "def test_train_split(df):\n",
    "    train_df = df['taux'].iloc[:round(len(df)*0.8)]\n",
    "    test_df = df['taux'].iloc[round(len(df)*0.8):]\n",
    "    train_df = pd.DataFrame(train_df)\n",
    "    test_df = pd.DataFrame(test_df)\n",
    "    return train_df,test_df\n",
    "#Functions to evaluate various Time series forecasting models\n",
    "\n",
    "def esEval(df):  #exponential smoothing Evaluation \n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    es_model = ExponentialSmoothing(train_df['taux'])\n",
    "    es_model_fit = es_model.fit()\n",
    "    # Make predictions on the test set\n",
    "    es_predictions = es_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1)\n",
    "    es_mse =  mean_squared_error(test_df['taux'], es_predictions)\n",
    "    es_mse = es_mse**0.5\n",
    "    return es_mse\n",
    "\n",
    "def arEval(df): #AutoRegressive Model\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    ar_model = ARIMA(train_df[\"taux\"], order=(1,0,0))\n",
    "    ar_model_fit = ar_model.fit()\n",
    "    ar_predictions = ar_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    ar = pd.DataFrame(ar_predictions)\n",
    "    ar_mse = mean_squared_error(test_df['taux'],ar)\n",
    "    ar_mse = ar_mse**0.5\n",
    "    return ar_mse\n",
    "\n",
    "def armaEval(df): #AutoRegressive Moving average model\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arma_model = ARIMA(train_df[\"taux\"], order=(1,0,1))\n",
    "    arma_model_fit = arma_model.fit()\n",
    "    arma_predictions = arma_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    arma = pd.DataFrame(arma_predictions)\n",
    "    arma_mse = mean_squared_error(test_df['taux'],arma)\n",
    "    arma_mse = arma_mse**0.5\n",
    "    return arma_mse\n",
    "\n",
    "def arimaEval(df): #AutoRegressive Integrated Moving Average\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arima_model = auto_arima(df['taux'],m=6,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    arima_model = ARIMA(train_df[\"taux\"], order=b[0])\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_predictions = arima_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    arima = pd.DataFrame(arima_predictions)\n",
    "    arima_mse = mean_squared_error(test_df['taux'],arima)\n",
    "    arima_mse = arima_mse**0.5\n",
    "    return arima_mse\n",
    "\n",
    "def order_extract(a : str):\n",
    "    string = a\n",
    "    order_start = string.index('order')\n",
    "    order_start_paren = string.index('(', order_start)\n",
    "    order_end_paren = string.index(')', order_start_paren)\n",
    "    order_string = string[order_start_paren+1:order_end_paren]\n",
    "    order = tuple(map(int, order_string.split(',')))\n",
    "    seasonal_order_start = string.index('seasonal_order')\n",
    "    seasonal_order_start_paren = string.index('(', seasonal_order_start)\n",
    "    seasonal_order_end_paren = string.index(')', seasonal_order_start_paren)\n",
    "    seasonal_order_string = string[seasonal_order_start_paren+1:seasonal_order_end_paren]\n",
    "    seasonal_order = tuple(map(int, seasonal_order_string.split(',')))\n",
    "    return order, seasonal_order\n",
    "\n",
    "def sarimaEval(df): #Seasonal Auto Regressive Integrated Moving Average\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arima_model = auto_arima(df['taux'],m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    global b \n",
    "    b =  order_extract(a)\n",
    "    model = sarimax.SARIMAX(train_df[\"taux\"], order=b[0], seasonal_order=b[1])  #Results from \"arima_model.summary()\". Look at Model and fill it in here\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1)\n",
    "    sarima = pd.DataFrame(predictions)\n",
    "    sarima_mse = mean_squared_error(test_df['taux'],sarima)\n",
    "    sarima_mse = sarima_mse**0.5\n",
    "    return sarima_mse\n",
    "\n",
    "# Methods to forecast data\n",
    "\n",
    "def esModel(df):  #exponential Smoothing model\n",
    "    series =  df['taux'].to_list()\n",
    "    alpha = 0.9\n",
    "    n_pred = 10\n",
    "    forecast = [series[0]] # initial forecast\n",
    "    for i in range(1, len(series)):\n",
    "        forecast.append((alpha * series[i] + (1 - alpha) * forecast[i-1]))\n",
    "    for i in range(n_pred):\n",
    "        forecast.append((alpha * forecast[-1] + (1 - alpha) * forecast[-1])) \n",
    "    pred = forecast[-10:]\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = pred\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    es = pred_df\n",
    "    return es\n",
    "\n",
    "def arModel(df):  #AutoRegressive Model\n",
    "    model = ARIMA(df[\"taux\"], order=(1,0,0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    ar = pred_df\n",
    "    return ar\n",
    "\n",
    "def armaModel(df): #ARMA model\n",
    "    model = ARIMA(df[\"taux\"], order=(1,0,1))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    arma = pred_df\n",
    "    return arma\n",
    "\n",
    "def arimaModel(df):  #ARIMA Model\n",
    "    arima_model = auto_arima(df['taux'],m=6,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    model = ARIMA(df[\"taux\"], order=b[0])\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    # Make predictions using the fitted model\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    arima = pred_df\n",
    "    return arima\n",
    "\n",
    "def sarimaModel(df):  #SARIMA Model\n",
    "    arima_model = auto_arima(df['taux'],m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    model = sarimax.SARIMAX(df[\"taux\"], order=b[0], seasonal_order=b[1])\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    # Make predictions using the fitted model\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    sarima = pred_df\n",
    "    return sarima\n",
    "\n",
    "def prediction(df):  # Use the prediction function of the model with the lowest MSE to make predictions\n",
    "    min_rmse = minRMSE(df)\n",
    "    '''print('RMSE using Exponential smoothing: ',esEval(df))\n",
    "    print('RMSE using AR Model: ', arEval(df))\n",
    "    print('RMSE using ARMA Model: ', armaEval(df))\n",
    "    print('RMSE using ARIMA Model: ', arimaEval(df))\n",
    "    print('RMSE using SARIMA Model: ', sarimaEval(df))'''\n",
    "    \n",
    "    if min_rmse == esEval(df):\n",
    "        #print('minimun RMSE was found using Exponential Smoothing ')\n",
    "        #print('min_mse =',min_rmse)\n",
    "        return esModel(df)\n",
    "    elif min_rmse == arEval(df):\n",
    "        #print('minimun RMSE was found using AR Model ')\n",
    "        #print('min_mse =',min_rmse)\n",
    "        return arModel(df)\n",
    "    elif min_rmse == armaEval(df):\n",
    "        #print('minimun RMSE was found using ARMA Model ')\n",
    "        #print('min_mse =',min_rmse)\n",
    "        return armaModel(df)\n",
    "    elif min_rmse == arimaEval(df):\n",
    "        #print('minimun RMSE was found using ARIMA Model ')\n",
    "        #print('min_mse =',min_rmse)\n",
    "        return arimaModel(df)\n",
    "    else:\n",
    "        #print('minimun RMSE was found using SARIMA Model ')\n",
    "        #print('min_mse =',min_rmse)\n",
    "        return sarimaModel(df)\n",
    "        \n",
    "def minRMSE(df):\n",
    "    min_rmse = min(esEval(df),arEval(df),armaEval(df),arimaEval(df),sarimaEval(df))\n",
    "    return min_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Functions to get the prediction output in .xlsx format. Prediction done of next 10 data points**<br>\n",
    "\n",
    "DO NOT RUN THE NEXT 4 CELLS UNLESS YOU WANT THE PREDICTION OUTPUT IN EXCEL FILE. \n",
    "\n",
    "FOR THE EASE OF THIS PROJECT IT HAS BEEN DONE ALREADY AND STORED IN **'prediction.xlsx'**.  THIS FILE CAN BE USED TO CLUSTED BASED ON DATES. <BR>\n",
    "\n",
    "The execution time of each of the next 4 Code chunks take up hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#BX Prediction\n",
    "\n",
    "pts = pd.read_excel(\"count.xlsx\", sheet_name=0)\n",
    "pts = pd.DataFrame(pts)\n",
    "pts = pts.query('Count > 40') \n",
    "pts['n_point']= pts['n_point'].astype(str)\n",
    "pts['RMSE'] = 0\n",
    "list1 = []\n",
    "for i in range (len(pts['Count'])):\n",
    "    d = pts['n_point'].iloc[i] #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "    m = 0 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "    df = SelectPoints(d,m)  \n",
    "    df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "    n = df['days before last pick up'].mode()\n",
    "    n = round(n[0])\n",
    "    newdf = prediction(df)\n",
    "    list1.append(newdf)\n",
    "    rmse = minRMSE(df)\n",
    "    print(pts['n_point'].iloc[i],rmse)\n",
    "    pts['RMSE'][i] = rmse\n",
    "\n",
    "dfl=[]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    dfl.append(pd.DataFrame(list1[i]))\n",
    "dff = pd.concat(dfl)\n",
    "dff.to_excel(\"BX Prediction.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Verre Prediction\n",
    "\n",
    "pts = pd.read_excel(\"count.xlsx\", sheet_name=1)\n",
    "pts = pd.DataFrame(pts)\n",
    "pts = pts.query('Count > 40') \n",
    "pts['n_point']= pts['n_point'].astype(str)\n",
    "pts['RMSE'] = 0\n",
    "list1 = []\n",
    "for i in range (len(pts['Count'])):\n",
    "    d = pts['n_point'].iloc[i] #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "    print(d)\n",
    "    m = 1 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "    df = SelectPoints(d,m)  \n",
    "    df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "    n = df['days before last pick up'].mode()\n",
    "    n = round(n[0])\n",
    "    newdf = prediction(df)\n",
    "    list1.append(newdf)\n",
    "    rmse = minRMSE(df)\n",
    "    print(pts['n_point'].iloc[i],rmse)\n",
    "    pts['RMSE'][i] = rmse\n",
    "\n",
    "dfl=[]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    dfl.append(pd.DataFrame(list1[i]))\n",
    "dff = pd.concat(dfl)\n",
    "dff.to_excel(\"Verre Prediction.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# OM Prediction\n",
    "\n",
    "pts = pd.read_excel(\"count.xlsx\", sheet_name=2)\n",
    "pts = pd.DataFrame(pts)\n",
    "pts = pts.query('Count > 40') \n",
    "pts['n_point']= pts['n_point'].astype(str)\n",
    "pts['RMSE'] = 0\n",
    "list1 = []\n",
    "for i in range (len(pts['Count'])):\n",
    "    d = pts['n_point'].iloc[i] #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "    m = 2 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "    df = SelectPoints(d,m)  \n",
    "    df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "    n = df['days before last pick up'].mode()\n",
    "    n = round(n[0])\n",
    "    newdf = prediction(df)\n",
    "    list1.append(newdf)\n",
    "    rmse = minRMSE(df)\n",
    "    print(pts['n_point'].iloc[i],rmse)\n",
    "    pts['RMSE'][i] = rmse\n",
    "dfl=[]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    dfl.append(pd.DataFrame(list1[i]))\n",
    "dff = pd.concat(dfl)\n",
    "dff.to_excel(\"OM Prediction.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Carton Prediction\n",
    "\n",
    "pts = pd.read_excel(\"count.xlsx\", sheet_name=3) #count.xlsx file contains the datapoints with number of observations at those point\n",
    "pts = pd.DataFrame(pts)\n",
    "pts = pts.query('Count > 40') #auto_arima function returns error when\n",
    "pts['n_point']= pts['n_point'].astype(str)\n",
    "pts['RMSE'] = 0\n",
    "list1 = []\n",
    "for i in range (len(pts['Count'])):\n",
    "    d = pts['n_point'].iloc[i] #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "    m = 3 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "    df = SelectPoints(d,m)  \n",
    "    df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "    n = df['days before last pick up'].mode()\n",
    "    n = round(n[0])\n",
    "    newdf = prediction(df)\n",
    "    list1.append(newdf)\n",
    "    rmse = minRMSE(df)\n",
    "    pts['RMSE'][i] = rmse\n",
    "dfl=[]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    dfl.append(pd.DataFrame(list1[i]))\n",
    "dff = pd.concat(dfl)\n",
    "dff.to_excel(\"Carton Prediction.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e1c2a393ed767c3342611fcc6c1d5beede818837142dce53ab7bb8937fa1df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}