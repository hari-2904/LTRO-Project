{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\"\"\"\n",
    "<br>@Project: LTRO Project - Team 4 - Tool for Forecasting,Clustering and VRP\n",
    "<br>@author: Harihareshwar Kumaravel\n",
    "<br>\"\"\"***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Script to forecast at a particular point for a type of waste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.tsa.statespace.sarimax as sarimax\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ///INPUTS///\n",
    "\n",
    "\n",
    "p = 'BX'\n",
    "q = 'Verre'\n",
    "r = \"OM\"\n",
    "s = \"Carton\"\n",
    "c = []\n",
    "loc = \"datas/Data cleared.xlsx\"  # Location where the cleaned data is present\n",
    "'''d = '191' #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "m = 1 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively'''\n",
    "\n",
    "# ///FUNCTIONS///\n",
    "\n",
    "def SelectPoints(point, m):\n",
    "    df = pd.read_excel(loc , sheet_name= m)\n",
    "    df = df[df['date'] >= '01-01-2018']\n",
    "    df['n_point']= df['n_point'].astype(str)\n",
    "    df = df[df[\"n_point\"] == point]\n",
    "    df['taux'] = df['taux'].fillna(value = np.mean(df['taux']))\n",
    "    df = df.query('taux <= 1.0')\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "    df['day'] = [i.day for i in df['date']]\n",
    "    df['month'] = [i.month for i in df['date']]\n",
    "    df['year'] = [i.year for i in df['date']]\n",
    "    df['week'] = [i.week for i in df['date']]\n",
    "    if m == 0:\n",
    "        for i in range(10):\n",
    "            c.append(p)\n",
    "    elif m == 1:\n",
    "        for i in range(10):\n",
    "            c.append(q) \n",
    "    elif m == 1:\n",
    "        for i in range(10):\n",
    "            c.append(r) \n",
    "    else:\n",
    "        for i in range(10):\n",
    "            c.append(s)\n",
    "    return df\n",
    "\n",
    "#n = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "#Select the point to be forecasted and the type of waste. \n",
    "'''df = SelectPoints(d,m)  \n",
    "df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "n = df['days before last pick up'].mean(skipna=True)'''\n",
    "\n",
    "#Spliting the data to evaluate the models using MSE(Mean Squared error)\n",
    "def test_train_split(df):\n",
    "    train_df = df['taux'].iloc[:round(len(df)*0.8)]\n",
    "    test_df = df['taux'].iloc[round(len(df)*0.8):]\n",
    "    train_df = pd.DataFrame(train_df)\n",
    "    test_df = pd.DataFrame(test_df)\n",
    "    return train_df,test_df\n",
    "\n",
    "#Functions to evaluate various Time series forecasting models\n",
    "\n",
    "def esEval(df):  #exponential smoothing Evaluation \n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    es_model = ExponentialSmoothing(train_df['taux'])\n",
    "    es_model_fit = es_model.fit()\n",
    "    # Make predictions on the test set\n",
    "    es_predictions = es_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1)\n",
    "    #es_predictions = es_model_fit.forecast(len(train_df)+len(test_df)-1)\n",
    "    es_mse =  mean_squared_error(test_df['taux'], es_predictions)\n",
    "    es_mse = es_mse**0.5\n",
    "    return es_mse\n",
    "\n",
    "def arEval(df): #AutoRegressive Model\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    ar_model = ARIMA(train_df[\"taux\"], order=(1,0,0))\n",
    "    ar_model_fit = ar_model.fit()\n",
    "    ar_predictions = ar_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    ar = pd.DataFrame(ar_predictions)\n",
    "    ar_mse = mean_squared_error(test_df['taux'],ar)\n",
    "    ar_mse = ar_mse**0.5\n",
    "    return ar_mse\n",
    "\n",
    "def armaEval(df): #AutoRegressive Moving average model\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arma_model = ARIMA(train_df[\"taux\"], order=(1,0,1))\n",
    "    arma_model_fit = arma_model.fit()\n",
    "    arma_predictions = arma_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    arma = pd.DataFrame(arma_predictions)\n",
    "    arma_mse = mean_squared_error(test_df['taux'],arma)\n",
    "    arma_mse = arma_mse**0.5\n",
    "    return arma_mse\n",
    "\n",
    "def arimaEval(df): #AutoRegressive Integrated Moving Average\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arima_model = auto_arima(df['taux'],start_p=1, max_P=1, start_q=1, max_Q=1, max_d=1, max_D=1, m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    arima_model = ARIMA(train_df[\"taux\"], order=b[0])\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_predictions = arima_model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1, dynamic=False)\n",
    "    arima = pd.DataFrame(arima_predictions)\n",
    "    arima_mse = mean_squared_error(test_df['taux'],arima)\n",
    "    arima_mse = arima_mse**0.5\n",
    "    return arima_mse\n",
    "\n",
    "def order_extract(a : str):\n",
    "    string = a\n",
    "    order_start = string.index('order')\n",
    "    order_start_paren = string.index('(', order_start)\n",
    "    order_end_paren = string.index(')', order_start_paren)\n",
    "    order_string = string[order_start_paren+1:order_end_paren]\n",
    "    order = tuple(map(int, order_string.split(',')))\n",
    "    seasonal_order_start = string.index('seasonal_order')\n",
    "    seasonal_order_start_paren = string.index('(', seasonal_order_start)\n",
    "    seasonal_order_end_paren = string.index(')', seasonal_order_start_paren)\n",
    "    seasonal_order_string = string[seasonal_order_start_paren+1:seasonal_order_end_paren]\n",
    "    seasonal_order = tuple(map(int, seasonal_order_string.split(',')))\n",
    "    return order, seasonal_order\n",
    "\n",
    "def sarimaEval(df): #Seasonal Auto Regressive Integrated Moving Average\n",
    "    test_train_split(df)\n",
    "    train_df = test_train_split(df)[0]\n",
    "    test_df = test_train_split(df)[1]\n",
    "    arima_model = auto_arima(df['taux'],start_p=1, max_P=1, start_q=1, max_Q=1, max_d=1, max_D=1, m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    global b \n",
    "    b =  order_extract(a)\n",
    "    model = sarimax.SARIMAX(train_df[\"taux\"], order=b[0], seasonal_order=b[1])  #Results from \"arima_model.summary()\". Look at Model and fill it in here\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(train_df), end=len(train_df)+len(test_df)-1)\n",
    "    sarima = pd.DataFrame(predictions)\n",
    "    sarima_mse = mean_squared_error(test_df['taux'],sarima)\n",
    "    sarima_mse = sarima_mse**0.5\n",
    "    return sarima_mse\n",
    "\n",
    "# Methods to forecast data\n",
    "\n",
    "def esModel(df):  #exponential Smoothing model\n",
    "    series =  df['taux'].to_list()\n",
    "    alpha = 0.9\n",
    "    n_pred = 10\n",
    "    forecast = [series[0]] # initial forecast\n",
    "    for i in range(1, len(series)):\n",
    "        forecast.append((alpha * series[i] + (1 - alpha) * forecast[i-1]))\n",
    "    for i in range(n_pred):\n",
    "        forecast.append((alpha * forecast[-1] + (1 - alpha) * forecast[-1])) \n",
    "    pred = forecast[-10:]\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = pred\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    es = pred_df\n",
    "    return es\n",
    "\n",
    "def arModel(df):  #AutoRegressive Model\n",
    "    model = ARIMA(df[\"taux\"], order=(1,0,0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    ar = pred_df\n",
    "    return ar\n",
    "\n",
    "def armaModel(df): #ARMA model\n",
    "    model = ARIMA(df[\"taux\"], order=(1,0,1))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    arma = pred_df\n",
    "    return arma\n",
    "\n",
    "def arimaModel(df):  #ARIMA Model\n",
    "    arima_model = auto_arima(df['taux'],m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    model = ARIMA(df[\"taux\"], order=b[0])\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    # Make predictions using the fitted model\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    arima = pred_df\n",
    "    return arima\n",
    "\n",
    "def sarimaModel(df):  #SARIMA Model\n",
    "    arima_model = auto_arima(df['taux'],m=7,trace=False)\n",
    "    a =str(arima_model.df_model)\n",
    "    b = order_extract(a)\n",
    "    model = sarimax.SARIMAX(df[\"taux\"], order=b[0], seasonal_order=b[1])\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.predict(start=len(df), end=len(df)+9, dynamic=False)\n",
    "    df1 = pd.DataFrame({'prediction': []})\n",
    "    df2 = pd.DataFrame()\n",
    "    a = df['n_point'].unique()[0]\n",
    "    b = []\n",
    "    # Make predictions using the fitted model\n",
    "    df1['prediction'] = predictions\n",
    "    timestamp = df['date'].iloc[-1] + timedelta(days = n)\n",
    "    for i in range(len(df1)):\n",
    "        df2 = df2.append({'date':timestamp},ignore_index=True)\n",
    "        timestamp = df2['date'].iloc[i] + timedelta(days=n)\n",
    "    pred_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    for i in range(len(df1)):\n",
    "        pred_df['date'][i] = pred_df['date'][i+len(df1)]\n",
    "    pred_df = pred_df.dropna()\n",
    "    for i in range(len(df1)):\n",
    "        b.append(a)\n",
    "    pred_df.insert(2, 'n_point', b)\n",
    "    #pred_df.insert(3, 'waste_type', c)\n",
    "    sarima = pred_df\n",
    "    return sarima\n",
    "\n",
    "def prediction(df):  # Use the prediction function of the model with the lowest MSE to make predictions\n",
    "    min_rmse = min(esEval(df),arEval(df),armaEval(df),arimaEval(df),sarimaEval(df))\n",
    "    print('RMSE using Exponential smoothing: ',esEval(df))\n",
    "    print('RMSE using AR Model: ', arEval(df))\n",
    "    print('RMSE using ARMA Model: ', armaEval(df))\n",
    "    print('RMSE using ARIMA Model: ', arimaEval(df))\n",
    "    print('RMSE using SARIMA Model: ', sarimaEval(df))\n",
    "    if min_rmse == esEval(df):\n",
    "        print('minimun RMSE was found using Exponential Smoothing ')\n",
    "        print('min_mse =',min_rmse)\n",
    "        return esModel(df)\n",
    "    elif min_rmse == arEval(df):\n",
    "        print('minimun RMSE was found using AR Model ')\n",
    "        print('min_mse =',min_rmse)\n",
    "        return arModel(df)\n",
    "    elif min_rmse == armaEval(df):\n",
    "        print('minimun RMSE was found using ARMA Model ')\n",
    "        print('min_mse =',min_rmse)\n",
    "        return armaModel(df)\n",
    "    elif min_rmse == arimaEval(df):\n",
    "        print('minimun RMSE was found using ARIMA Model ')\n",
    "        print('min_mse =',min_rmse)\n",
    "        return arimaModel(df)\n",
    "    else:\n",
    "        print('minimun RMSE was found using SARIMA Model ')\n",
    "        print('min_mse =',min_rmse)\n",
    "        return sarimaModel(df)\n",
    "        \n",
    "def minRMSE(df):\n",
    "    min_rmse = min(esEval(df),arEval(df),armaEval(df),arimaEval(df),sarimaEval(df))\n",
    "    return min_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***User Inputs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = '99-1' #Enter the point where the prediction has to be made. Make sure it is in 'str' format\n",
    "m = 0 # m = 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "\n",
    "df = SelectPoints(d,m)  \n",
    "df['days before last pick up'] = df['days before last pick up'].apply(pd.to_numeric, errors='coerce')\n",
    "n = df['days before last pick up'].mode()\n",
    "n = round(n[0])\n",
    "prediction(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\"Predictions.xlsx\" contains the predicted values for different types of wastes(inside the datas folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ///LIBRAIRIES///\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ///USER INPUTS///\n",
    "date = '2022-07-01' #Date on which the clustering need to be done\n",
    "m = 0 # Input type of waste - 0,1,2,3 for BX,Verre,OM,Carton respectively\n",
    "\n",
    "# ///OTHER VARIABLES///\n",
    "dist = pd.read_excel(\"Distances.xlsx\", sheet_name=1) #Distance from the depot to various location\n",
    "\n",
    "# ///FUNCTIONS///\n",
    "\n",
    "def clean_column(value):  #To clean the column to get int datatype\n",
    "    if isinstance(value, int):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            return value\n",
    "\n",
    "def Selectpoints(date):   #Selects datapoints from the predicted data for a single date\n",
    "    date = date\n",
    "    df = pd.read_excel(\"Predictions.xlsx\", sheet_name = m )\n",
    "    df['date']=pd.to_datetime(df['date'])\n",
    "    df['n_point'] = df['n_point'].astype(str)\n",
    "    df['day'] = [i.day for i in df['date']]\n",
    "    df['month'] = [i.month for i in df['date']]\n",
    "    df['year'] = [i.year for i in df['date']]\n",
    "    df['week'] = [i.week for i in df['date']]\n",
    "    df = df.loc[df['date'] == date]\n",
    "    df[['points', 'bin_no']] = df['n_point'].str.extract(r'(\\d+)-?(\\d*)')\n",
    "    df['points'] = df['points'].fillna(value=df['n_point'])\n",
    "    df = df.loc[:,('date','n_point','prediction','points','bin_no')]\n",
    "    #df = df.query('prediction >= 0.7')\n",
    "    df= df.reset_index()\n",
    "    df['points'] = df['points'].apply(clean_column)\n",
    "    df['dist'] = 0\n",
    "    for i in range (len(df)):\n",
    "        for j in range(len(dist)):\n",
    "            if df['points'][i] == dist['n_point'][j]:\n",
    "                df.loc[i, 'dist'] = dist.loc[j,'dist']\n",
    "    return df\n",
    "\n",
    "def OptimumClusters(df):\n",
    "    X = df[['dist','prediction']]\n",
    "\n",
    "    km = KMeans()\n",
    "    visualizer = KElbowVisualizer(km, k=(2,8)) #Number of available trucks is 7\n",
    "    \n",
    "    visualizer.fit(X)        # Fit the data to the visualizer\n",
    "    visualizer.show()        # Finalize and render the figure\n",
    "    return visualizer.elbow_value_\n",
    "\n",
    "def Clustering(df):  #Clusters the data based on prediction and distance from depot using k-means algorithm \n",
    "    X = df[['dist','prediction']]\n",
    "    km = KMeans(n_clusters = OptimumClusters(df))\n",
    "    y_predicted = km.fit_predict(X)\n",
    "\n",
    "    df['cluster'] = y_predicted\n",
    "    b = df['cluster'].unique()\n",
    "    a =[]\n",
    "    for i in range(len(b)):\n",
    "        c = df[df.cluster == i]\n",
    "        a.append(c['points'].unique())\n",
    "    data = []\n",
    "    for arr in a:\n",
    "        data.append(arr.tolist())\n",
    "    data = [[elem-1 for elem in sublist] for sublist in data]\n",
    "    return data\n",
    "\n",
    "df = Selectpoints(date)\n",
    "\n",
    "print(Clustering(df))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copy the above result and paste in 'loc' variable in the below chunk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Vehicle Routing Problem** <br> This here is treated as a TSP to get the optimum route of the clusters obtained previously. The routing may or maynot be optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2\n",
    "from ortools.constraint_solver import pywrapcp\n",
    "import pandas as pd\n",
    "\n",
    "#Enter the list of locations where VRP needs to be implemented\n",
    "\n",
    "#loc is the output from clustering\n",
    "loc = [[77, 197, 104, 159, 147, 151, 213, 146, 226, 227, 228, 165, 204], [4, 264, 61], [102, 90, 100, 98, 125, 91, 97, 241, 223, 121], [80, 160, 272]] #Paste the result from above chunk here to execute the VRP\n",
    "global b\n",
    "b = []\n",
    "\n",
    "def distMat(loc):\n",
    "    loc1 = loc.copy()\n",
    "    loc1 = np.insert(loc,0,328) # depot location\n",
    "    df = pd.read_excel(\"D:/Grenoble INP-SIE/Semester 3/Logistics and OR/PROJECTS/PROJECTS/LTRO_Project Files/Project Codes/Distances.xlsx\", sheet_name=2, header=None)\n",
    "    df = df.loc[loc1,loc1] # to extract distance matrix for specific locations alone\n",
    "    m = df.to_numpy() # to create a numpy array for distance matrix\n",
    "    return m\n",
    "\n",
    "def create_data_model(dmat):\n",
    "    \"\"\"Stores the data for the problem.\"\"\"\n",
    "    data = {}\n",
    "    data['distance_matrix'] = dmat\n",
    "    data['num_vehicles'] = 1\n",
    "    data['depot'] = 0\n",
    "    return data\n",
    "\n",
    "def print_solution(manager, routing, solution):\n",
    "    \"\"\"Prints solution on console.\"\"\"\n",
    "    ###print('Objective: {} metres'.format(solution.ObjectiveValue()))\n",
    "    index = routing.Start(0)\n",
    "    plan_output = '\\n'\n",
    "    route_distance = 0\n",
    "    global a\n",
    "    a = []\n",
    "    while not routing.IsEnd(index):\n",
    "        #plan_output += ' {} , '.format(manager.IndexToNode(index))\n",
    "        a.append(manager.IndexToNode(index))\n",
    "        previous_index = index\n",
    "        index = solution.Value(routing.NextVar(index))\n",
    "        route_distance += routing.GetArcCostForVehicle(previous_index, index, 0)\n",
    "    a.pop(0)\n",
    "    #plan_output += ' {}\\n'.format(manager.IndexToNode(index))\n",
    "    plan_output += 'Route distance: {}metres'.format(route_distance)\n",
    "    print(plan_output)\n",
    "    return a\n",
    "\n",
    "def vrp(loc):\n",
    "    pl_list = []\n",
    "    for i in range(len(loc)):\n",
    "        pl = loc[i]\n",
    "        pl_list.append(distMat(pl))\n",
    "    \n",
    "    for i in range(len(loc)):\n",
    "        \"\"\"Entry point of the program.\"\"\"\n",
    "        # Instantiate the data problem.\n",
    "        data = create_data_model(pl_list[i])\n",
    "\n",
    "        # Create the routing index manager.\n",
    "        manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']),\n",
    "                                            data['num_vehicles'], data['depot'])\n",
    "\n",
    "        # Create Routing Model.\n",
    "        routing = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "\n",
    "        def distance_callback(from_index, to_index):\n",
    "            \"\"\"Returns the distance between the two nodes.\"\"\"\n",
    "            # Convert from routing variable Index to distance matrix NodeIndex.\n",
    "            from_node = manager.IndexToNode(from_index)\n",
    "            to_node = manager.IndexToNode(to_index)\n",
    "            return data['distance_matrix'][from_node][to_node]\n",
    "\n",
    "        transit_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "\n",
    "        # Define cost of each arc.\n",
    "        routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)\n",
    "\n",
    "        # Setting first solution heuristic.\n",
    "        search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "        search_parameters.first_solution_strategy = (\n",
    "            routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC)\n",
    "\n",
    "        # Solve the problem.\n",
    "        solution = routing.SolveWithParameters(search_parameters)\n",
    "\n",
    "        # Print solution on console.\n",
    "        if solution:\n",
    "            print_solution(manager, routing, solution)\n",
    "\n",
    "        values = loc[i]\n",
    "        order = a\n",
    "        Order_visit = sort_list(values, order)\n",
    "        Order_visit.append(328)\n",
    "        Order_visit.insert(0,328)\n",
    "        Visit_Order(Order_visit)\n",
    "        print('Tour' + ' ' + str(i+1)+ ':' , Order_visit) \n",
    "    print ('\\n',b)\n",
    "\n",
    "def Visit_Order(Order_visit):\n",
    "    b.append(Order_visit)\n",
    "    \n",
    "def sort_list(values, order):\n",
    "    # Zip the values and order lists together\n",
    "    zipped_lists = zip(values, order)\n",
    "\n",
    "    # Sort the zipped list by the order of the elements in the second list\n",
    "    sorted_zipped_lists = sorted(zipped_lists, key=lambda x: x[1])\n",
    "\n",
    "    # Unzip the sorted list and return the first list (the values list)\n",
    "    sorted_values, _ = zip(*sorted_zipped_lists)\n",
    "\n",
    "    sorted_values = np.array(sorted_values)\n",
    "    sorted_values = sorted_values.tolist()\n",
    "    return sorted_values\n",
    "\n",
    "vrp(loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Final line is the result of VRP which can be used to visualize the route on that particular date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e1c2a393ed767c3342611fcc6c1d5beede818837142dce53ab7bb8937fa1df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
